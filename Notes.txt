- Was hebt mich bzw. meine Implementierung von anderen Umsetzungen ab? Wo liegt der Vorteil? Was mache ich besser/anders
- Warum gibt es eine analoge Lösung für ein eigentliches digitales Problem (Stichwort: Kamera)? digital UI --> analog Kamera --> digital Code
Da es zu Zeitaufwendig wäre die Modifikation der Pins selbstsändig zu machen möglich aber alles sehr filigran und die Firma die diese Produziert macht es nicht von selber,
man bräuchte neue Baupläne und Centkosten mehr für Liebherr weshalb dieser Ansatzt von der Oberen ebene verneint wurde. Deshalb Kamera um Display zu übertragen
- Warum Opencv? Mir war es wichtig eine Breite schnittstelle an Programmiersprachen unc crossplatform compatibilität zu haben sollte man nicht immer auf Windows bleiben. Außerdem ist opencv Open source und besitzt eine breite community die lizenzfrei immer und in zukunft zur verfügung steht. Aiuch ist die Firma dahinter eine non profit organisation.

Probleme:
- Bilder also Snapshots der Kamera sind Tagsüber mit Beleuchtung der Deckenlampe super jedoch nachts einfach sehr schlecht Lösung?: Belichtung bzw. der delay der Kamera bei Snapshots automatisch anpassen aber wenn ja wie welche Faktoren fast unmöglich zu berücksichtigen nur Tag/nacht oder auch wolkig wie soll man das immer alles prüfen generalisieren eventuell mit einer geschlossenen Kiste? oder die Bilder immer gleich erstellen und diese dann durch Algorithmus autohelligkeit immer auf einen Sollwert regeln
- Fokusstörung da immer auf Hintergrund fokusiert und Video langlaufend snapshotten sind die Ergebnisse auch von madiger Qualität
- Farbspektrum analysieren der Pixel ohne Toleranz unmöglich da man denkt es ist schwarz aber am Ende ist keines genau also = 0 schwarz deswegen muss mit Toleranz gearbeitet werden das fast schwarz oder eben auf alle anderen Farben angewendet auch funktioniert.
- Welches Featurematcher algorithmus nehmen? Da vergleich der vielen mit besseren da kostenlos usw.. siehe Folien Chapter:3 Seite 71 und ab weiviel matches es denn als sicher gilt auswerten (siehe nächstes Problem)
- wieviele feature best matches sind oder werden benötigt um eine gute klassifizierung zu bieten? laut ChatGPT: Threshold-Adaption in der Praxis: Einige Studien argumentieren, dass die Anzahl der benötigten Übereinstimmungen abhängig von der Szene und den Anforderungen variiert. In kontrollierten Umgebungen wird oft ein höherer Schwellenwert (z. B. 50 oder mehr Übereinstimmungen) gefordert, um eine hohe Zuverlässigkeit sicherzustellen, da die Bilddaten weniger durch externe Störfaktoren beeinflusst werden​ aber siehe auch diese Papers: 1. https://elib.dlr.de/97889/1/brisk_rm.pdf  2. https://isprs-archives.copernicus.org/articles/XL-1/371/2014/isprsarchives-XL-1-371-2014.pdf
- ocr probiert und halt bei allem brauchen wir Bilder mit vernünftiger Qualität problem von oben
- probleme im dunkler umgebung zwar gut da gleich aber display wenn nicht voll gefüllt nicht erkennbar deswegen lösung: Initialisierung kalibrierung mit ungleich schwarz um display mit pixel zuerkennen. auch werden die shots nicht zum richtigen verhältnis getroffen deswegen werden die 4 eckpunkte analysiert und auf die maße gesheeard
- werte mit RGB Tests waren kompliziert und nicht immer einfach -- bessere und auch mathematische herleitung gibt HSV da Hue Bereiche klar definiert sind einfachere handhabung deshalb änderung. Ebenso viel robuster gegen Lichtverhältnisse und  Keine komplexen Vergleiche wie if r > b and g > b, sondern klare Hue-Grenzen. Siehe: https://www.researchgate.net/publication/228895050_HSV-based_Color_Texture_Image_Classification_using_Wavelet_Transform_and_Motif_Patterns
	Achtung bei HSV Farbkreis rot kommt am Anfang und am Ende deswegen braucht man dafür zwei definierte Zonen siehe Bild auf: https://wisotop.de/hsv-und-hsl-farbmodell.php und: Achtung: OpenCV speichert HSV mit H-Werten zwischen 0-179!
																																																				Das heißt:

																																																				0° – 10° wird zu 0 – 5 in OpenCV
																																																				350° – 360° wird zu 175 – 179 in OpenCV
- Wenn kein Display erkannt wird bei der kalibrierung welche ungleich schwarze pixel anzeigt da ja im dunkeln keine ecken wenn das display schwarz ist anzeigt bzw erkannt werden kann soll ein error zurück gegeben werden und anstelle eckpunkte dann spezifischer flag abgespeichert werden was dann zukünftige aufrufe dazu verleitet das ganze bild zu nehmen ohne rücksicht auf displays usw.. zb count color mit ganzen übergebenem Bild --> Mit Canny Kantenerkennung für rechteck (Polygonapproximation) waren die ergebnisse am besten. explizit nach Kanten (starken Helligkeitsübergängen) via cv.Canny, statt nach der größten hellen Fläche.
- die Kombi Bristk und RANSAC funktioniert gut, wenn viele Feature Punkte erkannt werden (wie im Beispiel mit dem Gemälde Sternennacht). Es kann sein, dass für Icons mit sparse features eher ein semi-klassischer Ansatz wie z.B. Template Matching besser geeignet ist, wird aber noch getestet
- Leider wurde wegen sparse features nun template matching aber erweitert angewended multi-scale-template-matching das wird erreicht durch tests der verschiedneen größen des bildes durch abgleiche der Kanten (Canny) deshalb sind farben auch komplett irrelevant also funktionier trotzdem --> Einziger Nachteil er schaut ob Icon in Image vorhanden wenn es ein Icon gibt mit einem Blatt das UI hat aber ein multi bild wie Blatt und tropfen findet er das Blatt ja trotzdem also da müsste man dann schauen
- Multi-Scale_template-match has one serious problem: it’s important to keep in mind that template matching does not do a good job of telling us if an object does not appear in an image. Sure, we could set thresholds on the correlation coefficient, but in practice this is not reliable and robust. If you are looking for a more robust approach, you’ll have to explore keypoint matching was uns wieder zum Problem oben führt.. - the tresholds works only good when the images/icons are often at the same size which we have. So this is better than nothing with our sparse features

Präsentationsideen:
- Was wollen wir eigentlich erreichen - was ist das aktuelle Problem? Automatisierte Tests: Ist das was wir auf dem Display sehen das was wir erwarten da man es nicht anders auslesen kann da pixel nur hingeschickt werden aber nicht zurück gelesen werden können
- Kamera in abgedunkelter Box da Problematisch mit außenwelt einflüssen und andere Verhältnisse und nur mit Software die Bilder zu normalisieren war einfach nicht gut/möglich - da dann die detection nie scharf war
- OCR mit AI mit Beispiel von Chinesischen Straßenschildern von easyocr dass das mega gut funktioniert - nur einmal aktuelles Model runtergeladen funktioniert es auch offline - ohne Internet was für uns wichtig ist!
- zeige zuerst was man mit BRISK umsetzen kann und wie gut es mit vielen Features funktioniert + also Bild mit vergleich das BRISK beste ist da keine lizenz und unabhängig von  rotation, skalierung usw..
- Zeige mit Beispiel das mit Sternennacht van goth und mit Beispiel von uns Menschen mit Markanten gesichtszügen lassen sich auch viel einfacher vonuns erkennen z. B. große, lange Hexennase oder Markantes großes langes Kinn andersnfalls als ein typisch rundes Gesicht das wenig Eigenschaften hat
- Dann das Beispiel von Template Matching und Multi-scale-template matching bei sparse features nötig mit Beispiel wie das aussieht durch immer wiederkehrende skalierung des main Images bis nahe an Icon Bsp COD oder UI bei uns --> Problem halt wenn ähnliche Bilder wie Blatt mit Wassertropfen suche aber eigentlich nur nach Icon: Blatt

Meine Anforderungen:
- Modularer klar strukturierter Aufbau das jede Funktion auch eine eigene Methode hat die aufgerufen werden kann aber chronologisch gut funktioniert


Ablauf:
1. Einlesen eines Bildes mit flexibler Anpassung an Größe, Nähe und Neigung.
2. Erkennung und Extraktion des Displays (viereckige UI).
3. Analyse der Pixel pro Farbe und Histogramme pro Farbe.
4. OCR-Analyse, um Texte zu erkennen und zu prüfen, ob ein bestimmter Text vorhanden ist.
5. Prüfen, ob ein gegebenes Bild ein Teil des Bildschirms ist und an der richtigen Position erkannt wird.

Inhaltlich Paper:
- Abstract
- Project explainen
- Technologien/ algorithmen warum und wie sie funktionieren
- Was hebt mein Projekt von anderen ab oder Use-Case erklären (Automation)
- Evaluierung



# Breite und Höhe des Bildes extrahieren (openCV)
height, width = gray_img.shape[:2]
# Breite (width) und Höhe (height) anzeigen
print(f"width: {width} Pixel")
print(f"height: {height} Pixel")



Robustheit:
- auf einem geht es aber ich auf vielen Bildern teste
- 
Paper:
- Was soll das 20.000 rote pixel aber wenn wir soll ich die nutzerausgabe machen also reicht ihnen die pixel 
- habe farbräume in 2 oder 3dimensionale raum wo ist der bereich der als rot bewertet wird, woanders würde das so nicht gehen zb autonomes fahren eventuell mathematisch aufschreiben wie ich das mathematisch recherchieren kann also klassifijation in 3 bereiche formuliert als formel je nach pixelwert
schreiben warum und wie man auf den weg gekommen ist den man nun macht.

in our pprior experience it wasnt successfull because of.. pixel classification auf dieses weges entschieden wir usn für das einfügen einer farbtolleranz 

Begründen warum ich diesen weg wählen

von wir sprechen.

Liebherr gespräch feedback:
echte Pixel mit Display vergleichen bzw. setzen

- returnwerte von einzelnen teile möglich
- ocr KI offline möglich?
- software packages lizensen prüfen
